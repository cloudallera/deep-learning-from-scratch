{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#5.-오차역전파법\" data-toc-modified-id=\"5.-오차역전파법-1\">5. 오차역전파법</a></span><ul class=\"toc-item\"><li><span><a href=\"#5.1-계산-그래프\" data-toc-modified-id=\"5.1-계산-그래프-1.1\">5.1 계산 그래프</a></span><ul class=\"toc-item\"><li><span><a href=\"#[계산-그래프로-풀다]\" data-toc-modified-id=\"[계산-그래프로-풀다]-1.1.1\">[계산 그래프로 풀다]</a></span></li><li><span><a href=\"#[국소적-계산]\" data-toc-modified-id=\"[국소적-계산]-1.1.2\">[국소적 계산]</a></span></li><li><span><a href=\"#[왜-계산-그래프로-푸는가?]\" data-toc-modified-id=\"[왜-계산-그래프로-푸는가?]-1.1.3\">[왜 계산 그래프로 푸는가?]</a></span></li></ul></li><li><span><a href=\"#5.2-연쇄법칙\" data-toc-modified-id=\"5.2-연쇄법칙-1.2\">5.2 연쇄법칙</a></span><ul class=\"toc-item\"><li><span><a href=\"#[계산-그래프의-역전파]\" data-toc-modified-id=\"[계산-그래프의-역전파]-1.2.1\">[계산 그래프의 역전파]</a></span></li><li><span><a href=\"#[연쇄법칙이란?]\" data-toc-modified-id=\"[연쇄법칙이란?]-1.2.2\">[연쇄법칙이란?]</a></span></li><li><span><a href=\"#[연쇄법칙과-계산-그래프]\" data-toc-modified-id=\"[연쇄법칙과-계산-그래프]-1.2.3\">[연쇄법칙과 계산 그래프]</a></span></li></ul></li><li><span><a href=\"#5.3-역전파\" data-toc-modified-id=\"5.3-역전파-1.3\">5.3 역전파</a></span><ul class=\"toc-item\"><li><span><a href=\"#[덧셈-노드의-역전파]\" data-toc-modified-id=\"[덧셈-노드의-역전파]-1.3.1\">[덧셈 노드의 역전파]</a></span></li><li><span><a href=\"#[곱셈-노드의-역전파]\" data-toc-modified-id=\"[곱셈-노드의-역전파]-1.3.2\">[곱셈 노드의 역전파]</a></span></li><li><span><a href=\"#[사과-쇼핑의-예]\" data-toc-modified-id=\"[사과-쇼핑의-예]-1.3.3\">[사과 쇼핑의 예]</a></span></li></ul></li><li><span><a href=\"#5.4-단순한-계층-구현하기\" data-toc-modified-id=\"5.4-단순한-계층-구현하기-1.4\">5.4 단순한 계층 구현하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#[곱셈-계층]\" data-toc-modified-id=\"[곱셈-계층]-1.4.1\">[곱셈 계층]</a></span></li><li><span><a href=\"#[덧셈-계층]\" data-toc-modified-id=\"[덧셈-계층]-1.4.2\">[덧셈 계층]</a></span></li></ul></li><li><span><a href=\"#5.5-활성화-함수-계층-구현하기\" data-toc-modified-id=\"5.5-활성화-함수-계층-구현하기-1.5\">5.5 활성화 함수 계층 구현하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#[ReLU-계층]\" data-toc-modified-id=\"[ReLU-계층]-1.5.1\">[ReLU 계층]</a></span></li><li><span><a href=\"#[Sigmoid-계층]\" data-toc-modified-id=\"[Sigmoid-계층]-1.5.2\">[Sigmoid 계층]</a></span><ul class=\"toc-item\"><li><span><a href=\"#&lt;&lt;Sigmoid-역전파-흐름&gt;&gt;\" data-toc-modified-id=\"<<Sigmoid-역전파-흐름>>-1.5.2.1\">&lt;&lt;Sigmoid 역전파 흐름&gt;&gt;</a></span></li><li><span><a href=\"#&lt;&lt;Sigmoid-계층의-계산-그래프-간소화&gt;&gt;\" data-toc-modified-id=\"<<Sigmoid-계층의-계산-그래프-간소화>>-1.5.2.2\">&lt;&lt;Sigmoid 계층의 계산 그래프 간소화&gt;&gt;</a></span></li></ul></li></ul></li><li><span><a href=\"#5.6-Affine/Softmax-계층-구현하기\" data-toc-modified-id=\"5.6-Affine/Softmax-계층-구현하기-1.6\">5.6 Affine/Softmax 계층 구현하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#[Affine-계층]\" data-toc-modified-id=\"[Affine-계층]-1.6.1\">[Affine 계층]</a></span></li><li><span><a href=\"#[배치용-Affine-계층]\" data-toc-modified-id=\"[배치용-Affine-계층]-1.6.2\">[배치용 Affine 계층]</a></span></li><li><span><a href=\"#[Softmax-with-Loss-계층]\" data-toc-modified-id=\"[Softmax-with-Loss-계층]-1.6.3\">[Softmax-with-Loss 계층]</a></span></li></ul></li><li><span><a href=\"#5.7-오차역전파법-구현하기\" data-toc-modified-id=\"5.7-오차역전파법-구현하기-1.7\">5.7 오차역전파법 구현하기</a></span><ul class=\"toc-item\"><li><span><a href=\"#[신경망-학습의-전체-순서]\" data-toc-modified-id=\"[신경망-학습의-전체-순서]-1.7.1\">[신경망 학습의 전체 순서]</a></span></li><li><span><a href=\"#[오차역전파법을-적용한-신경망-구현하기]\" data-toc-modified-id=\"[오차역전파법을-적용한-신경망-구현하기]-1.7.2\">[오차역전파법을 적용한 신경망 구현하기]</a></span></li><li><span><a href=\"#[오차역전파법으로-구한-기울기-검증하기]\" data-toc-modified-id=\"[오차역전파법으로-구한-기울기-검증하기]-1.7.3\">[오차역전파법으로 구한 기울기 검증하기]</a></span></li><li><span><a href=\"#[오차역전파법을-사용한-학습-구현하기]\" data-toc-modified-id=\"[오차역전파법을-사용한-학습-구현하기]-1.7.4\">[오차역전파법을 사용한 학습 구현하기]</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 오차역전파법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞 장에서는 신경망의 가중치 매개변수의 기울기(정확히는 가중치 매개변수에 대한 손실함수의 기울기)는 수치 미분을 사용해 구했으나, <br>\n",
    "   수치 미분은 단순하고 구현하기도 쉽지만 계산 시간이 오래 걸리는 단점이 있음\n",
    "- **오차역전파법(backpropagation, backward propagation of errors)**은 가중치 매개변수에 대한 손실함수의 기울기를 효율적으로 계산하기 위한 방법\n",
    "- 오차역전파법을 이해하는 방법은 수식을 통한 방법 또는 계산 그래프를 이용한 방법이 있음(이번 장에서는 계산 그래프를 사용해 시각적으로 이해)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 계산 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **계산 그래프(Computational Graph)**는 계산 과정을 그래프로 나타낸 것\n",
    "- 그래프는 그래프 자료구조로, 복수의 **노드(Node)**와 **에지(Edge)**로 표현(노드 사이의 직선을 '에지'라고 함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [계산 그래프로 풀다]\n",
    "\n",
    "- 계산 그래프는 계산 과정을 노드와 화살표로 표현함\n",
    "- 노드는 원으로 표기하고, 원 안에 연산 내용을 기술\n",
    "- 계산 결과를 화살표 위에 적어서 각 노드의 계산 결과가 왼쪽에서 오른쪽으로 전해짐\n",
    "- 사과 지불 금액 계산 그래프 [그림 5.1]\n",
    "\n",
    ">![사과 계산 그래프 순전파](./images/0001.jpeg)\n",
    "\n",
    "- 사과와 귤의 지불금액 계산 그래프 [그림 5.2]\n",
    "\n",
    ">![사과와 귤 계산 그래프 순전파](./images/0002.jpeg)\n",
    "\n",
    "- **계산 그래프를 이용한 문제풀이 흐름**\n",
    "\n",
    "> 1. 계산 그래프를 구성한다.\n",
    "> 1. 그래프에서 계산을 왼쪽에서 오른쪽으로 진행한다.\n",
    "\n",
    "- 계산을 왼쪽에서 오른쪽으로 진행하는 단계를 **순전파(Forward Propagation)** 라고 함\n",
    "- 반대방향(오른쪽에서 왼쪽)으로 전파가 진행하는 단계를 **역전파(Backward Propagation)** 라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [국소적 계산]\n",
    "\n",
    "- 계산 그래프의 특징은 **\"국소적 계산\"을 전파함으로써 최종 결과를 얻는다**는 점에 있음\n",
    "- 국소적이란 **\"자신과 직접 관계된 작은 범위\"**라는 뜻\n",
    "- 국소적 계산은 결국 전체에서 어떤 일이 벌어지든 상관 없이 자신과 관계된 정보만으로 다음 결과(그 후의 결과)를 출력할 수 있음. <br>\n",
    "   각 노드는 자신과 관련된 계산 외에는 아무것도 신경 쓸 게 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [왜 계산 그래프로 푸는가?]\n",
    "\n",
    "1. 국소적 계산을 통해 전체가 아무리 복잡해도 각 노드에서는 단순한 계산에 집중하여 문제를 단순화할 수 있음\n",
    "1. 계산 그래프는 중간 계산 결과를 모두 보관할 수 있음\n",
    "1. 역전파를 통해 \"미분\"을 효율적으로 계산할 수 있음\n",
    "\n",
    ">예로, **\"사과 가격이 오르면 최종 금액에 어떤 영향을 끼치는지\"**를 알고 싶다면, <br>\n",
    "이 문제는 **\"사과 가격에 대한 지불 금액의 미분\"**을 구하는 문제에 해당함. <br>\n",
    "기호로 나타내면 사과 값을 $x$, 지불 금액을 $L$ 이라 했을 때, $\\frac{\\delta L}{\\delta x}$을 구하는 것. <br>\n",
    "**이 미분 값은 사과 값이 \"아주 조금\" 올랐을 때 지불 금액이 얼마나 증가하느냐를 표시한 것 이며,** <br>\n",
    "**\"사과 가격에 대한 지불 금액의 미분\" 같은 값은 계산 그래프에서 역전파를 하면 구할 수 있음**\n",
    "\n",
    "- **역전파에 의한 미분 값의 전달** [그림 5.3]\n",
    "\n",
    ">![역전파에 의한 미분 값의 전달](./images/0003.jpg)\n",
    "> - 역전파는 순전파와는 반대 방향의 화살표(굵은 선)로 그리며, 국소적 미분 값을 전달하고 그 미분 값은 화살표의 아래에 적음\n",
    "> - 위의 그림에서, \"사과 가격에 대한 지불 금액의 미분\" 값은 2.2라 할 수 있으며, <br>\n",
    "     사과 값이 아주 조금 오르면 최종 금액은 그 아주 작은 값의 2.2배 만큼 오른다는 뜻\n",
    "> - 소비세에 대한 지불 금액의 미분이나, 사과 개수에 대한 지불 금액의 미분도 구할 수 있으며, <br>\n",
    "     이러한 계산 시 중간까지 구한 미분 결과를 공유할 수 있어서 다수의 미분을 효율적으로 계산할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 연쇄법칙"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 국소적 미분을 전달하는 원리는 **\"연쇄법칙(Chain Rule)\"**에 따른 것임\n",
    "- **연쇄법칙은 계산 그래프의 역전파와 같음**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [계산 그래프의 역전파]\n",
    "\n",
    "- **$\\normalsize y = f(x)$ 계산 그래프의 역전파** [그림 5.4]\n",
    "\n",
    ">![계산 그래프의 역전파](./images/0004.jpg)\n",
    "> - 역전파의 계산 절차는 신호 $E$에 노드의 국소적 미분($\\large \\frac{\\delta y}{\\delta x}$)을 곱한 후 다음 노드로 전달 함\n",
    "> - 국소적 미분은 순전파 때의 y = f(x) 계산의 미분을 구한다는 것이며, 이는 x에 대한 y의 미분($\\large \\frac{\\delta y}{\\delta x}$)을 구한다는 뜻\n",
    "> - 가령, $y = f(x) = x^2$ 이라면, $\\large \\frac{\\delta y}{\\delta x}$는 $2x$ 가 되며, 이 국소적인 미분을 상류에서 전달된 값($E$)에 곱해서 앞쪽 노드로 전달 함\n",
    "> - 이러한 방식을 따르면 목표로 하는 미분 값을 효율적으로 구할 수 있다는 것이 이 전파의 핵심임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [연쇄법칙이란?]\n",
    "\n",
    "- 합성 함수란 여러 함수로 구성된 함수로, $\\normalsize z = (x + y)^2$ 은 다음 두 개의 식으로 구성됨\n",
    "\n",
    "> $\\large z = t^2 \\\\\n",
    "     \\large t = x + y$ &nbsp;&nbsp;&nbsp;&nbsp; --- \\[식 5.1]\n",
    "\n",
    "- 연쇄법칙은 합성 함수의 미분에 대한 성질이며, <br>\n",
    "   **연쇄법칙의 정의**는 **\"합성 함수의 미분은 합성 함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있다.\"** <br><br>\n",
    "\n",
    "- $\\Large \\frac{\\delta z}{\\delta x}$($x$에 대한 $z$의 미분)은  $\\Large \\frac{\\delta z}{\\delta t}$($t$에 대한 $z$의 미분)과 $\\Large \\frac{\\delta t}{\\delta x}$($x$에 대한 $t$의 미분)의 곱으로 나타낼 수 있음 ---- \\[식 5.2]\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta x} = \\frac{\\delta z}{\\delta t} \\frac{\\delta t}{\\delta x}$$ <br>\n",
    "\n",
    "- 위 식에서 $\\delta t$는 분모와 분자에서 서로 지울 수 있음\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta x} = \\frac{\\delta z}{\\not \\delta t} \\frac{\\not \\delta t}{\\delta x}$$ <br>\n",
    "\n",
    "- \\[식 5.1]에서 국소적 미분(편미분)을 구하면 아래와 같음 ---- \\[식 5.3]\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta t} = 2t \\\\\n",
    "\\large \\frac{\\delta t}{\\delta x} = 1$$ <br>\n",
    "> - $\\large \\frac{\\delta z}{\\delta t}$는 $2t$이고, $\\large \\frac{\\delta t}{\\delta x}$는 1 이며, 이는 미분 공식에서 해석적으로 구한 결과임\n",
    "\n",
    "- 최종적으로 구하고 싶은 $\\large \\frac{\\delta z}{\\delta x}$는 [식 5.3]에서 구한 두 미분을 곱해서 계산 함 ---- \\[식 5.4]\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta x} = \\frac{\\delta z}{\\delta t} \\frac{\\delta t}{\\delta x} = 2t \\cdot 1 = 2(x + y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [연쇄법칙과 계산 그래프]\n",
    "\n",
    "- [식 5.4]의 계산 그래프 : 순전파와 반대 방향으로 국소적 미분을 곱하여 전달 [그림 5.5]\n",
    "\n",
    ">![식 5.4의 계산 그래프](./images/0005.jpg)\n",
    "\n",
    "- \"$**2$\" 노드에서의 역전파는 입력이 $\\large \\frac{\\delta z}{\\delta z}$이며, 이에 극소적 미분인 $\\large \\frac{\\delta z}{\\delta t}$를 곱하고 다음 노드로 넘김 <br>\n",
    "   (순전파 시에는 입력이 $t$이고 출력이 $z$이므로 이 노드에서 국소적 미분은 $\\large \\frac{\\delta z}{\\delta t}$)\n",
    "- 역전파의 첫 신호인 $\\large \\frac{\\delta z}{\\delta z}$의 값은 1\n",
    "- 맨 왼쪽의 역전파는 연쇄법칙에 따르면, 아래 식이 성립되어 \"$x$에 대한 $z$의 미분\"이 됨\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta z} \\frac{\\delta z}{\\delta t} \\frac{\\delta t}{\\delta x} = \\frac{\\delta z}{\\delta t} \\frac{\\delta t}{\\delta x} = \\frac{\\delta z}{\\delta x}$$\n",
    "\n",
    "- 즉, **역전파가 하는 일은 연쇄법칙의 원리와 같다.**\n",
    "- [그림 5.5]에 [식 5.3]을 대입한 계산 그래프의 역전파 결과\n",
    "\n",
    "> ![계산 그래프 역전파 결과](./images/00051.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 역전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [덧셈 노드의 역전파]\n",
    "\n",
    "- $\\normalsize z = x + y$의 미분은 다음과 같이 해석적으로 계산할 수 있음 ---- \\[식 5.5]\n",
    "\n",
    ">$$\\large \\frac{\\delta z}{\\delta x} = 1 \\\\\n",
    "    \\large \\frac{\\delta z}{\\delta y} = 1$$\n",
    "\n",
    "- **덧셈 노드의 역전파**는 $\\large \\frac{\\delta z}{\\delta x}$와 $\\large \\frac{\\delta z}{\\delta y}$ 모두 1 이이서, **입력 값을 그대로 흘려보냄** [그림 5.6]\n",
    "\n",
    ">![덧셈 노드의 역전파](./images/0006.jpg)\n",
    "> - 상류에서 전해진 미분(이 예에서는 $\\Large \\frac{\\delta z}{\\delta t}$)에 $\\normalsize 1$을 곱하여 하류로 흘림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [곱셈 노드의 역전파]\n",
    "\n",
    "- $\\normalsize z = x \\: y$의 미분은 다음과 같이 해석적으로 계산할 수 있음 ---- [식 5.6]\n",
    "\n",
    "> $$\\large \\frac {\\delta z}{\\delta x} = y \\\\\n",
    "       \\large \\frac {\\delta z}{\\delta y} = x$$\n",
    "\n",
    "- 곱셈 노드의 역전파 계산 그래프 [그림 5.7]\n",
    "\n",
    "> ![곱셈 노드의 역전파](./images/0007.jpg)\n",
    "> - 곱셈 노드의 역전파는 상류의 값에 순전파 때의 입력 신호들을 \"서로 바꾼 값\"을 곱해서 하류로 보냄\n",
    "> - \"서로 바꾼 값\"이란 순전파 때 $x$ 였다면 역전파에서는 $y$, 순전파 때 $y$ 였다면 역전파에서는 $x$로 바꾼다는 의미\n",
    "> - 곱셈의 역전파는 순방향 입력 신호의 값이 필요하기 때문에 곱셈 노드를 구현할 때는 순전파의 입력 신호를 지유해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [사과 쇼핑의 예]\n",
    "\n",
    "- 사과의 가격, 사과의 개수, 소비세라는 세 변수 각각이 최종 금액에 어떻게 영향을 주는지 문제를 풀고자 함\n",
    "- 이는 \"사과 가격에 대한 지불 금액의 미분\", \"사과 개수에 대한 지불 금액의 미분\", \"소비세에 대한 지불 금액의 미분\"을 구하는 것임\n",
    "- 사과 쇼핑의 역전파 예 [그림 5.8]\n",
    "\n",
    ">![사과 쇼핑 역전파](./images/0008.jpeg)\n",
    "> - 곱셈 노드의 역전파에서는 상류의 값에 입력 신호를 서로 바꾼 값을 곱해서 하류로 흘림\n",
    "\n",
    "- 사과와 귤 쇼핑의 역전파 예 [그림 5.9]\n",
    "\n",
    ">![사과와 귤 쇼핑 역전파](./images/0009.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 단순한 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망을 구성하는 층(계층) 각각을 하나의 클래스로 구현(연산을 담당하는 노드를 클래스로 구현)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [곱셈 계층]\n",
    "\n",
    "- 모든 계층은 forward()와 backward()라는 공통의 메서드(인터페이스)를 갖도록 구현. forward()는 순전파, backward()는 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 계산 그래프 구현\n",
    "class MultiLayer():\n",
    "    def __init__(self):\n",
    "        self.x = None   # 역전파에 사용하기 위해, 순전파 때의 입력 값을 유지하기 위한 변수\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x * y\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):   # dout은 상류층의 순전파의 미분 값\n",
    "        dx = dout * self.y    # 상류에서 전달된 값에 x와 y를 바꿔서 곱한다.\n",
    "        dy = dout * self.x\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price : 220.00\n"
     ]
    }
   ],
   "source": [
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MultiLayer()\n",
    "mul_tax_layer = MultiLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "price = mul_tax_layer.forward(apple_price, tax)\n",
    "\n",
    "print(\"price : {:.2f}\".format(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dapple, dapple_num, dtax : 2.20, 110.00, 200.00\n"
     ]
    }
   ],
   "source": [
    "# 역전파\n",
    "dprice = 1\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "\n",
    "print(\"dapple, dapple_num, dtax : {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "    dapple, dapple_num, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [그림 5.8]의 곱셈의 역전파 그래프와 결과가 동일함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [덧셈 계층]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 덧셈 계층 계산 그래프 구현\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass      # 덧셈 노드에서는 순전파 때의 입력 값을 유지할 필요 없음(역전파에서 사용하지 않음)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = x + y\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1     # 상류에서 내려온 미분(dout) 값을 그대로 하류로 흘려 보냄\n",
    "        dy = dout * 1\n",
    "\n",
    "        return dx, dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price : 715\n",
      "dapple, dapple_num, dorange, dorange_num, dtax: 2.20, 110.00, 3.30, 165.00, 650.00\n"
     ]
    }
   ],
   "source": [
    "# 덧셈 계층과 곱셈 계층을 사용한 사과와 귤의 계산 그래프 구현\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "orange = 150\n",
    "orange_num = 3\n",
    "tax = 1.1\n",
    "\n",
    "# 계층들\n",
    "mul_apple_layer = MultiLayer()\n",
    "mul_orange_layer = MultiLayer()\n",
    "add_apple_orange_layer = AddLayer()\n",
    "mul_tax_layer = MultiLayer()\n",
    "\n",
    "# 순전파\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
    "orange_price = mul_orange_layer.forward(orange, orange_num)\n",
    "all_price = add_apple_orange_layer.forward(apple_price, orange_price)\n",
    "price = mul_tax_layer.forward(all_price, tax)\n",
    "\n",
    "# 역전파\n",
    "dprice = 1\n",
    "dall_price, dtax = mul_tax_layer.backward(dprice)\n",
    "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
    "dorange, dorange_num = mul_orange_layer.backward(dorange_price)\n",
    "\n",
    "print(\"price : {:.0f}\".format(price))\n",
    "print(\"dapple, dapple_num, dorange, dorange_num, dtax: {:.2f}, {:.2f}, {:.2f}, {:.2f}, {:.2f}\".format(\n",
    "    dapple, dapple_num, dorange, dorange_num, dtax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 필요한 계층을 만들고 순전파 메서드인 **forward()를 적절한 순서로 호출** (계산 그래프의 순전파 순서 대로 호출)\n",
    "- **순전파와 반대 순서로 역전파 메서드인 backward()를 호출하면 미분이 계산됨**\n",
    "- [그림 5.9]의 사과와 귤 쇼핑의 역전파 결과와 동일함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망을 구성하는 층(계층) 각각을 클래스 하나로 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ReLU 계층]\n",
    "\n",
    "- ReLU 수식 ---- \\[식 5.7]\n",
    "\n",
    "> $$\\normalsize y = \\begin{cases} x \\ \\ \\ \\ \\ (x > 0) \\\\ \n",
    "    0 \\ \\ \\ \\ \\ (x \\le 0) \\end{cases}$$\n",
    "\n",
    "- $x$에 대한 $y$의 미분 ---- \\[식 5.8]\n",
    "\n",
    "> $$\\normalsize \\frac{\\delta y}{\\delta x} = \\begin{cases} 1 \\ \\ \\ \\ \\ (x > 0) \\\\ \n",
    "   0 \\ \\ \\ \\ \\ (x \\le 0) \\end{cases}$$\n",
    "> - [식 5.8]에서와 같이 **순전파 때의 입력인 $x$가 $0$ 보다 크면 역전파는 상류의 값을 그대로 하류로 흘리고 ($1$을 곱해서 하류로 흘리고)**,\n",
    "> - **순전파 때 $x$가 $0$ 이하이면 역전파 때는 하류에 신호를 보내지 않음 ($0$을 보냄)**\n",
    "\n",
    "- ReLU 계층의 계산 그래프 [그림 5.10]\n",
    "\n",
    "> ![ReLU 계층 계산 그래프](./images/0010.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU 계층 구현 (forward() 함수와 backward() 함수는 numpy 배열을 인수로 받는 것으로 가정)\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        # True/False로 구성된 numpy 배열로, 순전파의 입력인 x의 원소 값이 0 이하인 인덱스는 True, 그 외의 인덱스는 False\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0  # self.mask가 True인 인덱스의 값을 0으로 변경\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # 순전파에서 만든 mask를 써서 mask가 True인 곳은 상류에서 전파된 dout을 0으로 변경\n",
    "        dout[self.mask] = 0\n",
    "\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  -0.5]\n",
      " [-2.   3. ]]\n",
      "[[False  True]\n",
      " [ True False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
    "print(x)\n",
    "\n",
    "mask = (x <= 0)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Sigmoid 계층]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시그모이드 함수 식 ---- \\[식 5.9]\n",
    "\n",
    "> $$\\normalsize y = \\frac{1}{1 + exp(-x)}$$\n",
    "> - $\\normalsize exp(-x)$는 지수함수 $\\large e^{-x}$를 의미\n",
    "\n",
    "- Sigmoid 계층의 계산 그래프(순전파)  ---- \\[그림 5.11]\n",
    "\n",
    "> ![시그모이드 순전파](./images/0011.jpg)\n",
    "> - \"$\\normalsize exp$\" 노드는 $\\normalsize y = exp(-x)$ 계산을 수행하고,\n",
    "> - \"$\\normalsize /$\" 노드는 $\\normalsize y = \\large \\frac{1}{x}$ 계산을 수행 ($\\normalsize x$는 국소적 미분의 입력값을 의미)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <<Sigmoid 역전파 흐름>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **1단계**\n",
    "\n",
    "> - $\\normalsize y = \\frac{1}{\\large x}$ 미분 식은, ---- \\[식 5.10]\n",
    "> $$\\normalsize \\frac{\\delta y}{\\delta x} = - \\frac{1}{x^2}$$ <br>\n",
    "> $$\\normalsize \\ \\ \\ = - y^2$$ <br>\n",
    "> - $\\normalsize y = \\frac{1}{\\large x}$의 $x$를 시그모이드 출력의 분모 부분으로 생각하면 시그모이드 출력의 미분에 그대로 적용 가능 <br>\n",
    "> $$\\normalsize y = \\frac{1}{1 + exp(-x)}$$ <br>\n",
    "> $$\\normalsize \\frac{\\delta y}{\\delta x} = - \\frac{1}{(1 + exp(-x))^2}$$ <br>\n",
    "> $$\\normalsize = - \\Big(\\frac{1}{1 + exp(-x)} \\Big)^2$$ <br>\n",
    "> $$\\normalsize = - y^2$$ <br>\n",
    "> - **\"$\\normalsize /$\" 노드는 상류의 예측값에 $-y^2$(순전파의 출력을 제곱한 후 마이너스를 붙인 값)을 곱해서 하류로 전달** <br><br>\n",
    "> $$\\normalsize - \\frac{\\delta L}{\\delta y} \\ y^2$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **2단계**\n",
    "\n",
    "> - **\"$\\normalsize +$\" 노드는 상류의 값을 여과 없이 하류로 내보냄** (1단계 출력과 동일함)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **3단계**\n",
    "\n",
    "> - **\"$\\normalsize exp$\" 노드는 $\\normalsize y = exp(-x)$ 연산을 수행** 하며, 그 미분은 다음과 같음 (밑이 $\\normalsize e$인 지수 함수의 도함수는 자기 자신) ---- \\[식 5.11] <br><br>\n",
    "> $$\\normalsize \\frac{\\delta y}{\\delta x} = exp(-x)$$ <br>\n",
    "> - \"$\\normalsize exp$\" 노드의 연산 결과는 상류의 예측값에 위의 값을 곱하여 아래와 같이 하류로 전달 <br><br>\n",
    ">  $$\\normalsize - \\frac{\\delta L}{\\delta y} \\ y^2 \\ exp(-x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **4단계**\n",
    "\n",
    "> - **\"$\\normalsize x$\" 노드는 순전파 때의 값을 서로 바꿔 곱함**. 이 예에서는 **-1을 곱 하면 됨** <br><br>\n",
    "> - 역전파의 최종 출력은,\n",
    "> $$\\normalsize \\frac{\\delta L}{\\delta y} \\ y^2 \\ exp(-x)$$ <br>\n",
    "> - 역전파 전체 과정을 포함한 Sigmoid 계층의 계산 그래프(순전파 &역전파) ---- \\[그림 5.12] <br>\n",
    "> ![단순한 시그모이드 역전파](./images/0012.jpg) <br>\n",
    "> - 위의 식 처럼, **$\\large \\frac{\\delta L}{\\delta y} \\normalsize y^2 exp(-x)$를 순전파의 입력 $x$와 출력 $y$ 만으로 계산할 수 있으며**, <br>\n",
    "    **계산 그래프의 중간 과정을 그룹화 하여 아래와 같이 단순한 \"sigmoid\" 노드 하나로 대체할 수 있음** ---- \\[그림 5.13]\n",
    "> ![단순한 시그모이드 역전파](./images/0013.jpg)\n",
    "> - \\[그림 5.12]와 \\[그림 5.13]의 결과는 똑같음. \\[그림 5.13]의 간소화 버전이 역전파 과정의 중간 계산들을 생략할 수 있어 더 효율적인 계산이라 말할 수 있음 <br>\n",
    "> - 노드를 그룹화 하여 sigmoid 계층의 세세한 내용을 노출하지 않고 입력과 출력에만 집중할 수 있다는 것도 중요한 포인트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <<Sigmoid 계층의 계산 그래프 간소화>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\large \\frac{\\delta L}{\\delta y} \\normalsize y^2 exp(-x)$는 다음처럼 정리해서 쓸 수 있음 ---- \\[식 5.12]\n",
    "> $$\\normalsize \\frac{\\delta L}{\\delta y} y^2 exp(-x) = \\frac{\\delta L}{\\delta y} \\frac{1}{(1 + exp(-x))^2} exp(-x)$$ <br>\n",
    "> $$\\normalsize = \\frac{\\delta L}{\\delta y} \\frac{1}{1 + exp(-x)} \\frac{exp(-x)}{1 + exp(-x)}$$ <br>\n",
    "> $$\\normalsize = \\frac{\\delta L}{\\delta y} \\ y \\ (1 - y)$$ <br>\n",
    "> - 참고로, $\\normalsize (1 - y)$로 축약되는 과정을 역으로 풀면, $\\normalsize 1$의 분모와 분자에 동일한 값$\\normalsize (1 + exp(-x))$을 곱한 후 분자 끼리 더함 <br><br>\n",
    "> $$\\normalsize (1 - y) = \\frac{\\not 1 + exp(-x)}{1 + exp(-x)} + \\frac{\\not -1}{1 + exp(-x)}$$ <br>\n",
    "> - 결론적으로, **sigmoid 계층의 역전파는 순전파의 출력($\\normalsize y$) 만으로 계산할 수 있음**\n",
    "\n",
    "- **Sigmoid 계층의 계산 그래프 : 순전파의 출력 $\\normalsize y$ 만으로 역전파 계산** ---- \\[그림 5.14]\n",
    "> ![Sigmoid 계층의 계산 그래프 최종](./images/0014.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid 계층 구현하기\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None     # 역전파 계산을 위한 순전파 출력값을 저장하기 위한 인스턴스 변수\n",
    "\n",
    "    def forward(self, x):\n",
    "        # sigmoid 출력 공식을 그대로 구현 (x는 numpy 배열이라는 전제)\n",
    "        out = 1 / (1 + np.exp(-x))\n",
    "        self.out = out                     # 역전파 계산을 위해 인스턴스 변수에 저장\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # [식 5.12]와 [그림 5.14]의 역전파 최종 출력 공식\n",
    "        dx = dout * self.out * (1 - self.out)\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Affine/Softmax 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Affine 계층]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망의 순전파에서는 가중치 신호의 총합을 계산하기 때문에 행렬의 내적을 사용함 : np.dot() 메서드 사용\n",
    "- 행렬의 내적 계산은 대응하는 차원의 원소 수를 일치시키는 게 핵심 ---- \\[그림 5.15]\n",
    "> ![행렬의 내적 대응 차원](./images/0015.jpg)\n",
    "- 신경망의 순전파 때 수행하는 **행렬의 내적**은 기하학에서는 **어파인 변환(Affine Transformation)** 이라고 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n",
      "[0.17713771 0.85793503 0.3310605 ]\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(2)\n",
    "W = np.random.rand(2, 3)\n",
    "B = np.random.rand(3)\n",
    "\n",
    "print(X.shape)\n",
    "print(W.shape)\n",
    "print(B.shape)\n",
    "\n",
    "Y = np.dot(X, W) + B\n",
    "\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Affine 계층의 계산 그래프(순전파)**\n",
    "> - 행렬의 내적을 계산하는 노드를 \"dot\"이라 정의\n",
    "> - Affine 계층 계산 그래프(순전파) : $\\matrix Y = np.dot(\\matrix X \\cdot \\matrix W) + \\matrix B$의 계산 그래프 (각 변수의 이름 위에 형상도 표기) ---- \\[그림 5.16]\n",
    "> ![Affine 계층 계산 그래프](./images/0016.jpg)\n",
    "> - $\\matrix X, W, B$는 행렬 : \"(2, )\"은 \"1행 2열\" 짜리 행렬의 Numpy 표현(열 벡터를 행렬로 표현한 것) 이며, N행 2열의 경우 (N, 2) 처럼 표현됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Affine 계층의 계산 그래프(역전파)**\n",
    "> - 행렬을 사용한 역전파도 행렬의 원소마다 전개해보면 스칼라값을 사용한 지금까지의 계산 그래프와 같은 순서로 생각할 수 있음 <br>\n",
    "> - **Affine 계층의 역전파 수식** ---- \\[식 5.13] <br><br>\n",
    "> $$\\frac{\\delta \\matrix L}{\\delta \\matrix X} = \\frac{\\delta \\matrix L}{\\delta \\matrix Y} \\cdot \\matrix W^T$$ <br>\n",
    "> $$\\frac{\\delta \\matrix L}{\\delta \\matrix W} = \\matrix X^T \\cdot \\frac{\\delta \\matrix L}{\\delta \\matrix Y}$$ <br>\n",
    "> - [\"벡터와 행렬에 대한 미분\" 참고 자료 1](http://darkpgmr.tistory.com/141), [\"벡터와 행렬에 대한 미분\" 참고 자료 2](https://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/fitting/matrix-derivative.ipynb) <br><br>\n",
    "> - $\\matrix W^T$의 T는 전치행렬을 뜻하며, 전치행렬은 $\\matrix W$의 $(i, j)$위치의 원소를 $(j, i)$ 위치로 바꾼 것 ---- \\[식 5.14] <br><br>\n",
    "> $$\\matrix W = \\begin{bmatrix} W_{11} W_{21} W_{31} \\\\\n",
    "        W_{12} W_{22} W_{32} \\end{bmatrix}$$ <br>\n",
    "> $$\\matrix W^T = \\begin{bmatrix} W_{11} W_{12} \\\\\n",
    "        W_{21} W_{22} \\\\\n",
    "        W_{31} W_{32} \\end{bmatrix}$$ <br>\n",
    "> - **Affine 계층의 역전파 계산 그래프 (변수는 다차원 배열)** ---- \\[그림 5.17]\n",
    "> ![Affine 계층 역전파 계산 그래프](./images/0017.jpg)\n",
    "> - **$\\matrix X$와 $\\large \\frac{\\delta L}{\\delta X}$가 같은 형상이고, $\\matrix W$와 $\\large \\frac{\\delta L}{\\delta W}$가 같은 형상임** (스칼라 함수인 $L$을 행렬 $\\matrix X$와 $\\matrix W$의 각 원소 마다 미분하는 형태) ---- \\[식 5.15] <br><br>\n",
    "> $$\\large \\matrix X = (x_0, x_1, \\cdots, x_n)$$ <br>\n",
    "> $$\\large \\frac{\\delta L}{\\delta X} = (\\frac{\\delta L}{\\delta x_0}, \\frac{\\delta L}{\\delta x_1}, \\cdots, \\frac{\\delta L}{\\delta x_n})$$ <br>\n",
    "> - 행렬 내적(\"dot\" 노드)의 역전파는 행렬의 대응하는 차원의 원소 수가 일치하도록 내적을 조립하여 구할 수 있음. <br>\n",
    "     예를들어, $\\large \\frac{\\delta L}{\\delta Y}$의 형상이 $(3, )$이고, $\\large \\frac{\\delta L}{\\delta X}$의 형상이 $(2, )$가 되도록 하는 $\\matrix W$의 형상 $(3, 2)$를 찾아낼 수 있음 ---- \\[그림 5.18]\n",
    "> ![행렬 내적 조립](./images/0018.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [배치용 Affine 계층]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치용 Affine 계층은 데이터 $N$개를 묶어 순전파 하는 경우의 Affine 계층으로, 기존 Affine 계층과 다른 부분은 형상이 $(N, 2)$가 된 것 뿐임 \n",
    "- **배치용 Affine 계층 계산 그래프** ---- \\[그림 5.19]\n",
    "> ![배치용 Affine 계층 계산 그래프](./images/0019.jpg)\n",
    "> - 순전파 때의 편향 덧셈은 $\\matrix X \\cdot \\matrix W$에 대한 편향 $\\matrix B$가 N개의 데이터 각각에 더해짐. <br>\n",
    "     결론적으로, 출력에 B의 N 배 만큼의 영향이 있음\n",
    "> - **역전파 때는 각 데이터의 역전파 값이 편향의 원소에 모여야 함.** <br>\n",
    "     미분의 정의(입력이 아주 조금 변동될 때 출력의 영향)에 따라 역전파 때는 각 데이터의 미분 값을 모두 더해야 함\n",
    "> - **편향의 역전파는 그 두 데이터에 대한 미분을 데이터 마다 더해서 구함.** <br>\n",
    "     numpy.sum( )으로 상류에서 전달된 미분값의 0번째 축(데이터를 단위로 하는 축, axis=0)에 대해서 총합을 구하면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 편향의 역전파\n",
    "dY = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dY)\n",
    "\n",
    "dB = np.sum(dY, axis=0)\n",
    "print(dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치용 Affine 계층 구현\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Softmax-with-Loss 계층]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Softmax 계층과 손실함수인 Cross-Entropy-Error 계층을 포함하여 **\"Softmax-with-Loss 계층\"** 으로 구현\n",
    "- 춣력층에서 사용하는 **Softmax 함수는 입력 값을 정규화(출력의 합이 1이 되도록 변형) 하여 출력함(출력 결과는 확률로 사용 가능함)**\n",
    "- 신경망의 학습에는 Softmax 계층이 필요하지만, 추론할 때는 일반적으로 Softmax 함수를 사용하지 않고 마지막 Affine 계층의 출력을 인식 결과로 이용함\n",
    "- 신경망에서 정규화 하지 않은 출력 결과(Softmax 바로 전의 Affine 계층의 출력)을 **점수**라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **간소화한 Softmax-with-Loss 계층 계산 그래프** ---- \\[그림 5.20]\n",
    "\n",
    "> ![Softmax-with-Loss 계층 계산 그래프](./images/fig5-30.png)\n",
    "> - 3클래스 분류로 가정하고, 이전 계층에서 3개의 입력(점수)을 받음. Softmax 계층의 입력은 $\\normalsize (a_1, a_2, a_3)$, 출력은 $\\normalsize (y_1, y_2, y_3)$\n",
    "> - Cross Entropy Error 계층은 Softmax 계층의 출력과 정답 레이블 $\\normalsize (t_1, t_2, t_3)$을 입력으로 받고, 손실 $\\normalsize L$을 출력\n",
    "> - Softmax-with-Loss 계층의 **순전파의 출력인 손실함수는** $\\normalsize L = - \\ (t_1 \\log {y_1} + t_2 \\log {y_2} + \\cdots + t_n \\log {y_n})$\n",
    "> - Softmax 계층의 **역전파는 $\\normalsize (y_1 - t_1, y_2 - t_2, y_3 - t_3)$ 이며, 이는 Softmax 계층의 출력과 정답 레이블의 차분과 같음.** <br>\n",
    "     신경망의 역전파에서는 이 차이인 오차가 앞 계층에 전해지는 것 임\n",
    "> - 신경망 학습의 목적은 신경망의 출력(Softmax의 출력)이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이며, <br>\n",
    "     신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달해야 함. $(y_1 - t_1, y_2 - t_2, y_3 - t_3)$ 라는 결과는 이를 잘 반영하고 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax-with-Loss 계층 구현\n",
    "class SoftmaWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **역전파 구현 시 참고사항**\n",
    "> - 역전파의 초기값은 가장 오른쪽 역전파의 값으로, $\\large \\frac{\\delta L}{\\delta L}$은 $1$ 임\n",
    "> - \"X\" 노드의 역전파는 순전파 시의 입력들의 값을 서로 바꿔서 상류의 미분에 곱하고 하류로 전달\n",
    "> - \"+\" 노드의 역전파는 상류에서 전해지는 미분을 그대로 출력\n",
    "> - \"log\" 노드의 역전파는 다음 식에 따름\n",
    "> $$\\normalsize y = \\log x$$ <br>\n",
    "> $$\\normalsize \\frac{\\delta y}{\\delta x} = \\frac{1}{x}$$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 오차역전파법 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [신경망 학습의 전체 순서]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 전제 : 신경망에는 적용 가능한 가중치와 편향이 있고, 이 **가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 \"학습\"이라 함**\n",
    "- **1단계(미니배치)** : 훈련 데이터 중 일부를 무작위로 가져옴. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표\n",
    "- **2단계(기울기 산출)** : 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함. <br>\n",
    "     기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시함. **오차역전파법**을 이용하면 느린 수치 미분과 달리 기울기를 효율적이고 빠르게 구할 수 있음\n",
    "- **3단계(매개변수 갱신)** : 가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "- **4단계(반복)** : 1~3단계를 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [오차역전파법을 적용한 신경망 구현하기]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TwoLayerNet 클래스의 인스턴스 변수**\n",
    "> - **params** : 딕셔너리 변수로 신경망의 **매개변수($W1, b1, W2, b2$)를 보관**\n",
    "> - **layers** : 순서가 있는 딕셔너리 변수로, 신경망의 계층을 보관. **각 계층을 순서대로 유지**(OrderedDict 클래스로 객체 생성)\n",
    "> - **lastLayer** : 신경망의 마지막 계층(**SoftmaxWithLoss 계층**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TwoLayerNet 클래스의 메서드**\n",
    "> - **__init__(self, input_size, hidden_size, output_size, weight_init_std)** : 초기화 수행 (weight_init_std는 가중치 초기화 시 정규분포의 스케일)\n",
    "> - **predict(self, x)** : 예측(추론)을 수행. x는 이미지 데이터\n",
    "> - **loss(self, x, t)** : 손실 함수의 값을 구함. x는 이미지 데이터, t는 정답 레이블\n",
    "> - **accuracy(self, x, t)** : 정확도를 구함\n",
    "> - **numerical_gradient(self, x, t)** : 가중치 매개변수의 기울기를 수치 미분 방식으로 구함\n",
    "> - **gradient(self, x, t)** : 가중치 매개변수의 기울기를 오차역전파법으로 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차역전파법을 적용한 2층 신경망 구현\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "from collections import OrderedDict\n",
    "from common.gradient import numerical_gradient\n",
    "from common.layers import *\n",
    "import numpy as np\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):     # x는 입력 데이터, t는 정답 레이블\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1:\n",
    "            t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        def loss_W(W): return self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Affine1'].dW\n",
    "        grads['b1'] = self.layers['Affine1'].db\n",
    "        grads['W2'] = self.layers['Affine2'].dW\n",
    "        grads['b2'] = self.layers['Affine2'].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **신경망의 계층을 OrderedDict 객체(순서가 있는 딕셔너리)에 보관**\n",
    "> - **순전파 때는 각 계층을 추가한 순서대로 forward() 메서드를 호출** 하기만 하면 처리가 완료됨\n",
    "> - **역전파 때는 계층을 추가한 반대 순서로 호출** 하기만 하면 됨\n",
    "> - Affine 계층과 ReLU 계층이 각자의 내부에서 순전파와 역전파를 처리하고 있기 때문에, 각 계층을 올바른 순서로 연결한 다음 호출해주기만 하면 됨\n",
    "- 신경망의 구성 요소를 \"계층\"으로 구현(\"계층\"으로 모듈화) 한 덕분에 **더 깊은 층을 구현하고 싶다면, 단순히 필요한 만큼 계층을 추가 만 하면 됨**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [오차역전파법으로 구한 기울기 검증하기]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기울기를 구할 때, 오차역전파법을 이용해 해석적으로 구하는 방법은 매개변수가 많아도 효율적으로 계산할 수 있으나, 수치 미분 방식은 느린 반면 구현하기 쉬움\n",
    "- 수치 미분 방식은 구현하기 쉬워서 버그가 숨어 있기 어려운 반면, 오차역전파법은 구현하기 복잡해서 종종 실수를 할 수 있음\n",
    "- **기울기 검증(gradient check)** : 수치 미분 방식의 결과와 오차역전파법의 결과를 비교하여 오차역전파법을 제대로 구현 했는지 검증하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: 1.8726913035405138e-07\n",
      "b1: 1.2500382484764367e-06\n",
      "W2: 1.1941204462753382e-12\n",
      "b2: 1.196820380213598e-10\n"
     ]
    }
   ],
   "source": [
    "# 오차역전파법 기울기 검증\n",
    "# 위에서 작성한 코드(오차역전파법을 적용한 2층 신경망 구현)를 two_layer_net.py로 저장해야 함\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "from two_layer_net import TwoLayerNet\n",
    "from dataset.mnist import load_mnist\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:100]\n",
    "t_batch = t_train[:100]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average(np.abs(grad_backprop[key] - grad_numerical[key]))\n",
    "    print(key + \": \" + str(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [오차역전파법을 사용한 학습 구현하기]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 번째 학습 loss, train_acc, test_acc : 2.302, 0.095, 0.098\n",
      "600 번째 학습 loss, train_acc, test_acc : 0.197, 0.906, 0.909\n",
      "1200 번째 학습 loss, train_acc, test_acc : 0.176, 0.925, 0.925\n",
      "1800 번째 학습 loss, train_acc, test_acc : 0.186, 0.932, 0.933\n",
      "2400 번째 학습 loss, train_acc, test_acc : 0.145, 0.945, 0.945\n",
      "3000 번째 학습 loss, train_acc, test_acc : 0.126, 0.951, 0.950\n",
      "3600 번째 학습 loss, train_acc, test_acc : 0.117, 0.957, 0.955\n",
      "4200 번째 학습 loss, train_acc, test_acc : 0.061, 0.962, 0.957\n",
      "4800 번째 학습 loss, train_acc, test_acc : 0.081, 0.965, 0.960\n",
      "5400 번째 학습 loss, train_acc, test_acc : 0.039, 0.968, 0.961\n",
      "6000 번째 학습 loss, train_acc, test_acc : 0.073, 0.970, 0.964\n",
      "6600 번째 학습 loss, train_acc, test_acc : 0.051, 0.971, 0.966\n",
      "7200 번째 학습 loss, train_acc, test_acc : 0.031, 0.974, 0.968\n",
      "7800 번째 학습 loss, train_acc, test_acc : 0.044, 0.976, 0.968\n",
      "8400 번째 학습 loss, train_acc, test_acc : 0.133, 0.977, 0.968\n",
      "9000 번째 학습 loss, train_acc, test_acc : 0.024, 0.977, 0.968\n",
      "9600 번째 학습 loss, train_acc, test_acc : 0.071, 0.980, 0.970\n",
      "9999 번째 학습 loss, train_acc, test_acc : 0.091, 0.980, 0.970\n"
     ]
    }
   ],
   "source": [
    "# 오차역전파법을 사용한 신경망 학습 구현\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.pardir)\n",
    "from two_layer_net import TwoLayerNet\n",
    "from dataset.mnist import load_mnist\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 오차역전파법으로 기울기를 구한다.\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grads[key]\n",
    "    \n",
    "    # 손실함수 계산 및 진행경과 저장\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 학습 진행 경과 저장 및 출력\n",
    "    if (i % iter_per_epoch == 0) or (i == (iters_num - 1)):\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"{:.0f} 번째 학습 loss, train_acc, test_acc : {:.3f}, {:.3f}, {:.3f}\".format(i, loss, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 손실함수 변화 추이 그래프\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15,4))\n",
    "for iters_num, ax in zip([10000, 1000], axes):\n",
    "    ax.plot(train_loss_list[:iters_num])\n",
    "    ax.set_title(\"{}-Iterations Loss\".format(iters_num))\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_xlim(0, iters_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 18)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 데이터와 시험 데이터에 대한 정확도 추이\n",
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "\n",
    "ax.plot(train_acc_list, c=\"b\")\n",
    "ax.plot(test_acc_list, \"--\", c=\"r\")     # 시험 데이터에 대한 정확도는 점선으로 표시(Red 색상)\n",
    "ax.set_title(\"훈련 데이터와 학습 데이터 정확도 추이\")\n",
    "ax.set_xlabel(\"Accuracy\")\n",
    "ax.set_ylabel(\"Epochs\")\n",
    "ax.set_ylim(0.0, 1.0)\n",
    "ax.set_xlim(0, len(train_acc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "730px",
    "left": "1035.203125px",
    "top": "110px",
    "width": "258.796875px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
