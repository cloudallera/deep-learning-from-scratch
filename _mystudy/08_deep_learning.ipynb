{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#8.-딥러닝\" data-toc-modified-id=\"8.-딥러닝-1\">8. 딥러닝</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.1-더-깊게\" data-toc-modified-id=\"8.1-더-깊게-1.1\">8.1 더 깊게</a></span><ul class=\"toc-item\"><li><span><a href=\"#[더-깊은-신경망으로]\" data-toc-modified-id=\"[더-깊은-신경망으로]-1.1.1\">[더 깊은 신경망으로]</a></span></li><li><span><a href=\"#[정확도를-더-높이려면]\" data-toc-modified-id=\"[정확도를-더-높이려면]-1.1.2\">[정확도를 더 높이려면]</a></span></li><li><span><a href=\"#[깊게-하는-이유]\" data-toc-modified-id=\"[깊게-하는-이유]-1.1.3\">[깊게 하는 이유]</a></span></li></ul></li><li><span><a href=\"#8.2-딥러닝의-초기-역사\" data-toc-modified-id=\"8.2-딥러닝의-초기-역사-1.2\">8.2 딥러닝의 초기 역사</a></span><ul class=\"toc-item\"><li><span><a href=\"#[이미지넷]\" data-toc-modified-id=\"[이미지넷]-1.2.1\">[이미지넷]</a></span></li><li><span><a href=\"#[VGG]\" data-toc-modified-id=\"[VGG]-1.2.2\">[VGG]</a></span></li><li><span><a href=\"#[GoogleNet]\" data-toc-modified-id=\"[GoogleNet]-1.2.3\">[GoogleNet]</a></span></li><li><span><a href=\"#[ResNet]\" data-toc-modified-id=\"[ResNet]-1.2.4\">[ResNet]</a></span></li></ul></li><li><span><a href=\"#8.3-더-빠르게(딥러닝-고속화)\" data-toc-modified-id=\"8.3-더-빠르게(딥러닝-고속화)-1.3\">8.3 더 빠르게(딥러닝 고속화)</a></span><ul class=\"toc-item\"><li><span><a href=\"#[연산-정밀도와-비트-줄이기]\" data-toc-modified-id=\"[연산-정밀도와-비트-줄이기]-1.3.1\">[연산 정밀도와 비트 줄이기]</a></span></li></ul></li><li><span><a href=\"#8.4-딥러닝의-활용\" data-toc-modified-id=\"8.4-딥러닝의-활용-1.4\">8.4 딥러닝의 활용</a></span><ul class=\"toc-item\"><li><span><a href=\"#[사물-검출]\" data-toc-modified-id=\"[사물-검출]-1.4.1\">[사물 검출]</a></span></li><li><span><a href=\"#[분할]\" data-toc-modified-id=\"[분할]-1.4.2\">[분할]</a></span></li><li><span><a href=\"#[사진-캡션-생성]\" data-toc-modified-id=\"[사진-캡션-생성]-1.4.3\">[사진 캡션 생성]</a></span></li></ul></li><li><span><a href=\"#8.5-딥러닝의-미래\" data-toc-modified-id=\"8.5-딥러닝의-미래-1.5\">8.5 딥러닝의 미래</a></span><ul class=\"toc-item\"><li><span><a href=\"#[이미지-스타일(화풀)-변환]\" data-toc-modified-id=\"[이미지-스타일(화풀)-변환]-1.5.1\">[이미지 스타일(화풀) 변환]</a></span></li><li><span><a href=\"#[이미지-생성]\" data-toc-modified-id=\"[이미지-생성]-1.5.2\">[이미지 생성]</a></span></li><li><span><a href=\"#[자율-주행]\" data-toc-modified-id=\"[자율-주행]-1.5.3\">[자율 주행]</a></span></li><li><span><a href=\"#[Deep-Q-Network(강화학습)]\" data-toc-modified-id=\"[Deep-Q-Network(강화학습)]-1.5.4\">[Deep Q-Network(강화학습)]</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 더 깊게"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [더 깊은 신경망으로]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **손글씨 숫자를 인식하는 심층 CNN**\n",
    "\n",
    "![솔글씨 숫자 인식 심층 CNN](./images/fig8-1.png)\n",
    "\n",
    "> - 위의 합성곱 계층은 3 x 3 크기의 작은 필터를 사용하고, 층이 깊어지면서 채널 수가 16, 16, 32, 32, 64, 64로 늘어남\n",
    "> - 활성화 함수는 ReLU를 사용하고, 풀링 계층을 추가하여 중간 데이터의 공간 크기를 점차 줄여감. 마지막 단의 완전연결 계층에는 드롭아웃 계층을 사용함\n",
    "> - 가중치 초기값은 He 초기값을 사용, 가중치 매개변수 갱신에는 Adam을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손글씨 숫자를 인식하는 심층 CNN 구현\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=10):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초기값 (\"He\" 초기값)\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 테스트 정확도 : 0.9935\n"
     ]
    }
   ],
   "source": [
    "# 손글씨 숫자를 인식하는 심층 CNN 훈련\n",
    "from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "# from deep_convnet import DeepConvNet     # 위의 DeepConvNet 클래스를 deep_convnet.py로 저장 후 import\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "network = DeepConvNet()\n",
    "\n",
    "# 학습 시간이 오래 걸림\n",
    "# trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "#                   epochs=20, mini_batch_size=100,\n",
    "#                   optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "#                   evaluate_sample_num_per_epoch=1000)\n",
    "# trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "# network.save_params(\"deep_convnet_params.pkl\")\n",
    "# print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 기 학습된 가중치 매개변수를 읽어들이기 (기 학습된 가중치 매개변수를 이용하여 최종 Test Accuracy 만 확인)\n",
    "network.load_params('deep_convnet_params.pkl')\n",
    "\n",
    "# Test Accuracy 확인\n",
    "test_acc = network.accuracy(x_test, t_test)\n",
    "print(\"최종 테스트 정확도 : {:.4f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [정확도를 더 높이려면]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[\"What is the class of this image?\" 웹 사이트](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html)**에 다양한 데이터셋을 대상으로 그동안 논문 등에서 발표한 기법들의 정확도 순위가 정리되어 있음\n",
    "- 위의 웹사이트에 상위 기법들을 참고하면 정확도를 더 높일 수 있는 기술이나 힌트를 발견할 수 있음. <br>\n",
    "   **앙상블 학습, 학습률 감소, 데이터 확장** 등이 정확도 향상에 도움이 됨\n",
    "- MNIST 데이터셋에 대해서는 층을 아주 깊게 하지 않고도 최고 수준의 결과가 나옴. <br>\n",
    "   손글씨 숫자 문제는 비교적 단순해서 신경망의 표현력을 극한까지 높일 필요가 없음\n",
    "- **데이터 확장(Data Augmentation)** 은 입력 이미지(훈련 이미지)를 알고리즘을 동원해 인위적으로 확장함\n",
    "> - 입력 이미지를 회전하거나, 세로로 이동하는 등 미세한 변화를 주어 이미지의 개수를 늘리는 것임\n",
    "> - 이는 데이터가 많지 않을 때 특히 효과적인 수단임\n",
    "> - 데이터 확장은 **변형** 외에도, **이미지의 일부를 잘라내는 crop** 이나, **좌우를 뒤집는 flip** 등이 있음\n",
    "> - 일반적인 이미지에는 **밝기 등의 외형 변화**나 **확대/축소 등의 스케일 변화**도 효과적임\n",
    "> - 데이터 확장을 통원해 훈련 이미지의 개수를 늘릴 수 있다면 딥러닌의 인식 수준을 개선할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [깊게 하는 이유]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 층을 깊게 하는 것의 중요성은 ILSVRC로 대표되는 대규모 이미지 인식 대회의 결과에서 파악할 수 있음\n",
    "- **층을 깊게 할 때의 이점은 신경망의 매개변수 수가 줄어듬**\n",
    "    - 층을 깊게 한 신경망은 깊지 않은 경우보다 적은 매개변수로 같은(혹은 그 이상) 수준의 표현력을 달성할 수 있음\n",
    "    - 5 X 5 합성곱 연산 1회는 3 X 3 합성곱 연산을 2회 수행하여 대체할 수 있음\n",
    "    - 5 X 5 합성곱 연산의 매개변수는 5 * 5 = 25인 반면, 3 X 3 합성곱 연산 2회는 2 * 3 * 3 = 18로 매개변수 수는 층을 반복할수록 적어짐. <br>\n",
    "       그 개수의 차이는 층이 깊어질수록 커짐\n",
    "\n",
    "![5 X 5 연산](./images/fig8-5.png)\n",
    "![3 X 3 연산](./images/fig8-6.png)\n",
    "\n",
    "- **작은 필터를 겹쳐 신경망을 깊게 할 때의 장점은 매개변수 수를 줄여 넓은 수용 영역(receptive field)을 소화할 수 있다**는 데 있음 <br>\n",
    "   (수용 영역은 뉴런에 변화를 일으키는 국소적인 공간 영역임) <br>\n",
    "   층을 거듭하면서 ReLU 등의 활성화 함수를 합성곱 계층 사이에 끼움으로써 신경망의 표현력이 개선됨 <br>\n",
    "   이는 활성화 함수가 신경망에 \"비선형\" 힘을 가하고, 비선형 함수가 겹치면서 더 복잡한 것도 표현할 수 있게 되기 때문임\n",
    "- **학습의 효율성**도 층을 깊게 하는 것의 이점이며, 층을 깊게 함으로써 학습 데이터의 양을 줄여 학습을 고속으로 수행할 수 있음\n",
    "- **층을 깊게 하면 정보를 계층적으로 전달할 수 있다는 점도 중요** <br>\n",
    "   신경망을 깊게 하면 학습해야 할 문제를 계층적으로 분해할 수 있으며, 각 층이 학습해야 할 문제를 더 단순한 문제로 대체할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 딥러닝의 초기 역사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ILSVRC 2012년 대회에서 AlexNet이 압도적인 성적으로 우승"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [이미지넷]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지넷(ImageNet)은 100만 장이 넘는 이미지를 담고 있는 데이터셋이며, <br>\n",
    "   다양한 종류의 이미지를 포함하며 각 이미지에는 레이블(클래스 이름)이 붙어 있음\n",
    "- **[ILSVRC](http://www.image-net.org/challenges/LSVRC/)**(ImageNet Large Scale Visual Recognition Challenge) 대회 분류 부문에서는 1,000개의 클래스를 제대로 분류하는지를 겨룸\n",
    "- 2012년 AlexNet 이후 선두는 항상 딥러님 방식이며, <br>\n",
    "   2015년에는 150층이 넘는 심층 신경망인 ResNet이 오류율을 3.5%까지 낮춤(일반적인 인간의 인식능력을 넘어섬)\n",
    "---\n",
    "- 이미지넷이 제공하는 거대한 데이터셋으로 학습한 가중치 값들은 실제 제품에 활용해도 효과적이고, 또 많이들 그렇게 이용하고 있음\n",
    "- **전이 학습(transfer learning) 이라고 해서, 학습된 가중치(혹은 그 일부)를 다른 신경망에 복사한 다음, 그 상태로 재학습(fine tuning)을 수행함** <br>\n",
    "   전이 학습은 보유한 데이터셋이 적을 때 특히 유용한 방법임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [VGG]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGG는 합성곱 계층과 풀링 계층으로 구성되는 기본적인 CNN 이며, **구성이 간단하여 응용하기 좋음**\n",
    "- **3X3의 작은 필터를 사용한 합성곱 계층을 연속으로 거침**. 2~4회 연속으로 풀링 계층을 두어 크기를 절반으로 줄이는 처리를 반복함\n",
    "\n",
    "![VGG](./images/fig8-9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [GoogleNet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GoogleNet은 세로 방향 길이 뿐 아니라 가로 방향도 깊다는 점이 특징\n",
    "- GoogleNet에는 가로 뱡향에 \"폭\"이 있으며, 이를 **인셉션 구조**라 함\n",
    "- 인셉션 구조는 **크기가 다른 필터를 여러 개 적용하여 그 결과를 결합** 한 구조이며, 이 인셉션 구조를 하나의 빌딩 블록(구성요소)으로 사용함\n",
    "\n",
    "![인셉션 구조](./images/fig8-11.png)\n",
    "\n",
    "- **1 X 1 크기의 필터를 사용** 한 합성곱 계층을 많은 곳에 사용하며, 이는 **채널 쪽으로 크기를 줄이는 것으로 매개변수 제거와 고속 처리에 기여함**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [ResNet]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝의 학습에서는 층이 지나치게 깊으면 학습이 잘 되지 않고, 오히려 성능이 떨어지는 경우도 있음\n",
    "- ResNet에서는 그런 문제를 해결하기 위해서 **스킵 연결(skip connection)** 을 도입\n",
    "- **스킵 연결이란 입력 데이터를 합성곱 계층을 건너뛰어 출력에 바로 더하는 구조임** <br>\n",
    "   단축 경로가 없었다면 두 합성곱 계층의 출력이 F(x)가 되나, 스킵 연결로 인해 F(x) + x가 되는게 핵심 <br>\n",
    "   스킵 연결은 층이 깊어져도 학습을 효율적으로 할 수 있도록 해주는데, 이는 **역전파 때 스킵 연결이 신호 감쇠를 막아주기 때문**\n",
    "\n",
    "![스킵 연결](./images/fig8-12.png)\n",
    "\n",
    "- ResNet은 합성곱 계층을 2개 층마다 건너뛰면서 층을 깊게 함. 150층 이상으로 해도 정확도가 오르는 모습을 확인할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 더 빠르게(딥러닝 고속화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝 고속화라는 주제는 **대량의 단일 곱셈-누산**을 어떻게 고속으로 효율적으로 계산하느냐는 것임\n",
    "- GPU는 병렬 수치 연산을 고속으로 처리할 수 있음\n",
    "- 딥러닝 프레임워크 대부분은 GPU(Graphics Processing Unit)을 활용해 대량의 연산을 고속으로 처리할 수 있음\n",
    "- 다수의 GPU와 컴퓨터를 이용한 분산 학습을 지원하는 딥러닝 프레임워크들이 나타나고 있음 <br>\n",
    "   분산 학습에서도 계산을 어떻게 분산 시키느냐는 몹시 어려운 문제이며, 이런 어려운 문제는 프레임워크에 맞기는 것이 좋음\n",
    "\n",
    "---\n",
    "### [연산 정밀도와 비트 줄이기]\n",
    "\n",
    "- 계산 능력 외에도 메모리 용량과 대역폭 등이 딥러닝 고속화에 병목이 될 수 있음\n",
    "- 대량의 가중치 매개변수와 중간 데이터를 메모리에 저장애햐 하며, GPU(혹은 CPU)의 버스를 흐르는 데이터가 많아져서 한계를 넘어서면 병목이 됨\n",
    "- 네트워크로 주고받는 데이터의 비트 수는 최소로 만드는 것이 바람직함\n",
    "- 다행히 **딥러닝은 높은 수치 정밀도(수치를 몇 비트로 표현하느냐)를 요구하지 않음**. 이는 신경망의 중요한 성질 중 하나로, 신경망의 견고성에 따른 특성임 <br>\n",
    "   신경망은 입력 이미지에 노이즈가 조금 썩여 있어도 출력 결과가 잘 달라지지 않는 강건함을 보여줌. <br>\n",
    "   이런 강건함 때문에 덕분에 신경망을 흐르는 데이터를 퇴화 시켜도 출력에 주는 영향은 적음\n",
    "- **딥러닝은 16비트 반정밀도(half-precision) 만 사용해도 학습에 문제가 없다고 알려져 있음** <br>\n",
    "   python에서는 일반적으로 64비트 배정밀도 부동소수점 수를 사용함 <br>\n",
    "   numpy는 16비트 반정밀도 부동소수점도 지원함\n",
    "- 가중치와 중간 데이터를 1비트로 표현하는 \"Binarized Neural Networks\" 라는 방법도 등장했음. 특히, 딥러닝을 임베디드용으로 이용할 때 중요한 주제임"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 딥러닝의 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [사물 검출]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **사물 검출은 이미지 속에 담긴 사물의 위치와 종류(클래스)를 알아내는 기술**\n",
    "- 사물 인식은 이미지 전체를 대상으로 했는데, <br>\n",
    "   사물 검출에서는 이미지 어딘가에 있을 사물의 위치까지 알아내야 하며, 한 이미지에 여러 사물이 존재할 수도 있음\n",
    "- **R-CNN(Regions with Convolutional Network)**\n",
    "\n",
    "![R-CNN](./images/fig8-18.png)\n",
    "\n",
    "- 먼저 사물이 위치한 영역을 찾아내고, 추출한 각 영역에 CNN을 적용하여 클래스를 분류함\n",
    "- 이미지를 사각형으로 변형하거나, 분류할 때 서포트 벡터 머신(SVM)을 사용하는 등 실제 처리 흐름은 다소 복잡함\n",
    "- 후보 영역 추출(사물처럼 보이는 물체를 찾아 처리)에는 컴퓨터 비전 분야에서 발전해온 다양한 기법을 사용할 수 있음 <br>\n",
    "   (R-CNN 논문에서는 Selective Search 기법을 사용)\n",
    "- 후보 영역 추출까지 CNN으로 처리하는 Faster R-CNN 기법도 등장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [분할]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 분할이란 이미지를 픽셀 수준에서 분류하는 문제임\n",
    "- 픽셀 단위로 객체마다 채색되 지도 데이터(supervised data)를 사용해 학습함\n",
    "- 추론할 때 입력 이미지의 모든 픽셀을 분류함\n",
    "- **FCN(Fully Convolutional Network)** : 단 한 번의 forward 처리로 모든 픽셀의 클래스를 분류해주는 기법\n",
    "\n",
    "![FCN](./images/fig8-20.png)\n",
    "\n",
    "- 사물 인식에서 사용한 신경망의 완전연결 계층에서는 중간 데이터의 공간 볼륨(다차원 형태)를 1차원으로 변환하여 한 줄로 늘어서 노드들이 처리했으나, <br>\n",
    "   완전연결 계층을 같은 기능을 하는 합성곱 계층으로 바꿈으로써,<br>\n",
    "   FCN에서는 공간 볼륨을 유지한 채 마지막 출력까지 처리할 수 있음\n",
    "- FCN은 마지막에 공간 크기를 확대하는 처리를 도입했다는 것도 특징 <br>\n",
    "   이 확대 처리로 인해 줄어든 중간 데이터를 입력 이미지와 같은 크기까지 단번에 확대할 수 있음 <br>\n",
    "   FCN의 마지막에 수행하는 확대는 이중 선형 보간(bilinear interpolation)에 의한 선형 확대 이며, <br>\n",
    "   **역합성곱(deconvolution)** 연산으로 구현함\n",
    "- 완전연결 계층을 CNN으로 대체하기 위해서는 합성곱 계층의 필터 수를 완전연결 계층의 출력 노드수 만큼 만들면 됨 <br>\n",
    "   (필터의 형상은 완전연결 계층의 입력 데이터 형상과 같게 만듦)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [사진 캡션 생성]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 딥러닝으로 사진 캡션을 생성하는 방법으로는 **NIC(Neural Image Caption)** 모델이 대표적임\n",
    "- **NIC는 심층 CNN과 자연어를 다루는 순환 신경망(Recurrent Neural Network)으로 구성됨**\n",
    "\n",
    "![NIC](./images/fig8-22.png)\n",
    "\n",
    "- RNN은 순환적 관계를 갖는 신경망으로 자연어나 시계열 데이터 등의 연속된 데이터를 다룰 때 많이 활용함\n",
    "- 사진이나 자연어와 같은 여러 종류의 정보를 조합하여 처리하는 것을 **멀티모달 처리(multimodal processing)** 라고 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 딥러닝의 미래"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [이미지 스타일(화풀) 변환]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"A Neural Algorithm of Artistic Style\"** 논문 : 두 이미지(하나는 콘텐츠 이미지, 다른 하나는 스타일 이미지)를 입력해서 새로운 그림을 생성\n",
    "- 네트워크의 중간 데이터가 콘텐츠 이미지의 중간 데이터와 비슷해지도록 학습함\n",
    "- 스타일 이미지의 화풍을 흡수하기 위해 \"스타일 행렬\" 이라는 개념을 도입함. <br>\n",
    "   스타일 행력의 오차를 줄이도록 학습하여 입력 이미지를 스타일 이미지의 화풍과 비슷해지도록 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [이미지 생성]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대량의 이미지를 사용하여 학습하기 하지만, 학습이 끝난 후에는 아무런 입력 이미지 없이도 새로운 그림을 그려냄\n",
    "- **DCGAN**(Deep Convolutional Generative Adversarial Network)은 이미지를 생성하는 과정을 모델화함\n",
    "    - DCGAN 기술의 핵심은 **생성자(Generator)와 식별자(Discriminator)** 로 불리는 2개의 신경망을 이용\n",
    "    - 생성자가 진짜와 똑같은 이미지를 생성하고,\n",
    "    - 식별자는 그것이 진짜인지(생성자가 생성한 이미지인지, 아니면 실제로 촬영된 이미지인지)를 판정함\n",
    "    - 생성자는 더 정교한 가짜 이미지 생성 능력을 학습하고, 식별자는 더 정확하게 간파할 수 있는 능력을 학습함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [자율 주행]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자율 주행은 다양한 기술(주행 경로를 정하는 경로 계획$^{path plan}$ 기술과 카메라나 레이저 등의 탐사 기술 등)을 모아 구현하고 있지만,\n",
    "   그중에서도 **주위 환경을 올바르게 인식하는 기술이 가장 중요한 문제임**\n",
    "- **SegNet** 이라는 CNN 기반 신경망은 주변 환경을 정확하게 인식해냄\n",
    "\n",
    "![이미지 분할](./images/fig8-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Deep Q-Network(강화학습)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **강화학습(reinforcement learning)** : 사람이 시행착오를 격으며 배우듯 컴퓨터도 시행착오 과정에서 스스로 학습하게 하려는 분야\n",
    "- 강화학습에서는 에이전트라는 것이 환경에 맞게 행동을 선택하고, 그 행동에 의해서 환경이 변한다는 게 기본적인 틀\n",
    "- 환경이 변화하면 에이전트는 어떠한 보상을 얻음. 강화학습의 목적은 에이전트의 행동 지침을 더 나은 보상을 받는 쪽으로 바로잡는 것임\n",
    "- 보상은 정해진 것이 아니라 \"예상 보상\" 임. 보상이 항상 명확한 것은 아니며, 어떤 상황에서 행동을 선택한 것이냐에 따라 보상은 달라질 수 있음\n",
    "- 딥러닝을 사용한 강화학습 중 **Deep Q-Network(DQN)** 라는 방법이 있음\n",
    "    - Q학습이라는 강화학습 알고리즘을 기초로 함\n",
    "    - Q학습에서는 최적 행동 가치 함수로 최적인 행동을 정함\n",
    "    - 이 함수를 딥러닝(CNN)으로 비슷하게 흉내 내어 사용하는 것이 DQN임\n",
    "    - DQN에서 사용하는 CNN은 게임 영상 프레임(4개의 연속한 프레임)을 입력하여, <br>\n",
    "      최종적으로 게임을 제어하는 움직임(조지스틱 이동량이나 버튼 조작 여부)에 대하여 각 동작의 \"가치\"를 출력함\n",
    "\n",
    "![DQN](./images/fig8-27.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
